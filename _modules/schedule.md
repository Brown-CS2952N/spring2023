---
title: Paper reading list and presenters
---

Jan 31, Tue
: [Course Overview](https://drive.google.com/file/d/17ykUcxQlraQLcQMIdlgUJMfBBF0ZieMN/view?usp=sharing) ([Slides](https://docs.google.com/presentation/d/1VSeMBMoJypRIVydBp8Q7DdAg_M1ZNinD/edit?usp=sharing&ouid=107174457905760692415&rtpof=true&sd=true))
  : Chen Sun
: 1. (Background) [How to Read a CS Research Paper](http://www2.cs.uregina.ca/~pwlfong/CS499/reading-paper.pdf) by Philip Fong
  1. (Background) [How to do research](http://people.csail.mit.edu/billf/publications/How_To_Do_Research.pdf) by Bill Freeman
  1. (Background) [How to do write a good paper](https://billf.mit.edu/sites/default/files/documents/cvprPapers.pdf) by Bill Freeman
  1. (Background) [How to speak (video)](https://www.youtube.com/watch?v=Unzc731iCUY) by Patrick Winston


Feb. 2, Thu
: [Deep Learning Recap](https://drive.google.com/file/d/1UFBSsH8lt56oL7iG2NHrRwnXRVzHg8pG/view?usp=share_link) ([Slides](https://docs.google.com/presentation/d/1kmvUO4ebot_Mz7-IbH3SspvZZgJN1x3m/edit?usp=sharing&ouid=107174457905760692415&rtpof=true&sd=true))
  : Chen Sun
: 1. (Background) [Novelty in Science](https://perceiving-systems.blog/en/news/novelty-in-science) by Michael Black
  1. (Background) [Everything is Connected: Graph Neural Networks](https://arxiv.org/abs/2301.08210)


Feb. 6, Mon
: **Due**{: .label .label-purple} [Presentation signup sheet](https://forms.gle/4CnTJGqKKx3vMWr89)


Feb. 7, Tue
: [Learning with Various Supervision](https://drive.google.com/file/d/1rbTVV1EtDcQEkauEazafymQxgpmq5puK/view?usp=sharing) ([Slides](https://drive.google.com/file/d/1nTfL79Sx_taukBb_Txh727HDGLcnW6eV/view?usp=sharing))
  : Chen Sun
: 1. (Background) [How to grow a mind: Statistics, structure, and abstraction](https://wiki.santafe.edu/images/e/e1/HowToGrowAMind%282011%29Tenebaum_J.pdf)
  1. (Background) [ICLR Debate with Leslie Kaelbling (video)](https://www.youtube.com/watch?v=veG8S5rqKIE)
  1. (Background) Learning with not Enough Data by Lilian Weng ([part1](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/) / [part2](https://lilianweng.github.io/posts/2022-02-20-active-learning/))


Feb. 9, Thu
: [The Bitter Lesson](https://drive.google.com/file/d/1clMnHW9TAhvDdlCGBZILE0dIDdZRAbqA/view?usp=share_link) ([Reading survey](https://forms.gle/LodhW8oy1tM4YJ2V8) / [Slides](https://docs.google.com/presentation/d/1z-gv9VkRKuGJcPYTuVxy0ybeT3PppoJaoowmsy-jx0c/edit?usp=sharing))
  : Amina, Ilija, and Raymond
: 1. [Revisiting Unreasonable Effectiveness of Data in the Deep Learning Era](https://arxiv.org/abs/1707.02968)
  1. [Unbiased Look at Dataset Bias](https://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf)
  1. (Background) [The bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
  1. (Background) [The Unreasonable Effectiveness of Data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)
  1. (Background) [Exploring Randomly Wired Neural Networks for Image Recognition](https://arxiv.org/abs/1904.01569)
  1. (Background) [NAS evaluation is frustratingly hard](https://arxiv.org/abs/1912.12522)


Feb. 14, Tue
: [Semi-supervised Learning](https://drive.google.com/file/d/1RQL2hrCI9wYZ_n_BRJYpGmmyWxz4lQGd/view?usp=sharing) ([Reading survey](https://forms.gle/S8hdDzNAcZ5FgfTb7) / [Slides](https://docs.google.com/presentation/d/1R-y6WaBqKcEdMTBaw_tTHdE3s-JCcJ0Wr1N0Tom6WgI/edit?usp=sharing))
  : Rosella, Patrick, Lingyu, and Michael Freeman
: 1. [Mean teachers are better role models](https://arxiv.org/abs/1703.01780)
  1. [MixMatch: A Holistic Approach to Semi-Supervised Learning](https://arxiv.org/abs/1905.02249)
  1. (Presentation) [Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning](https://arxiv.org/pdf/1704.03976.pdf)
  1. (Presentation) [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://arxiv.org/pdf/2001.07685.pdf)
  1. (Background) [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)
  1. (Background) [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)
  1. (Background) [Transfer Learning in a Transductive Setting](https://papers.nips.cc/paper/2013/hash/3295c76acbf4caaed33c36b1b5fc2cb1-Abstract.html)


Feb. 16, Thu
: Transfer Learning ([Reading survey](https://forms.gle/2e3ab7oAUYqaYfsV9) / [Slides](https://docs.google.com/presentation/d/124RgSJd3MlLB6JkZSoVwA-wNAcoNxxOlLKTsRYdkA-c/edit?usp=sharing))
  : Wasiwasi, Abubakarr, Yiqing, and Jacob
: 1. [Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks](https://leon.bottou.org/publications/pdf/cvpr-2014.pdf)
  1. [Transfusion: Understanding Transfer Learning for Medical Imaging](https://arxiv.org/abs/1902.07208)
  1. (Background) [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)
  1. (Background) [Rethinking Pre-training and Self-training](https://arxiv.org/abs/2006.06882)
  1. (Background) [A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark](https://arxiv.org/abs/1910.04867)
  1. (Background) [Rethinking ImageNet Pre-training](https://arxiv.org/abs/1811.08883)


Feb. 21, Tue
: _University holiday, no class_


Feb. 23, Thu
: Few-shot and In-context Learning
  : Sheridan, Shreyas, and Zhuo
: 1. [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080)
  1. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
  1. (Background) [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)
  1. (Background) [Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175)
  1. (Background) [Learning to Learn (Blog)](https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/)
  1. (Background) [Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?](https://arxiv.org/abs/2003.11539)
  1. (Background) [Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.pdf)


Feb. 28, Tue
: Multitask Learning
  : Noah, Alexander Meyerowitz, Pinyuan
: 1. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (Section 1, 2, 4)
  1. [A Generalist Agent](https://arxiv.org/abs/2205.06175)
  1. (Background) [Intelligence without representation](https://people.csail.mit.edu/brooks/papers/representation.pdf)
  1. (Background) [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)
  1. (Background) [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/abs/1804.08328)
  1. (Background) [UberNet: Training a Universal Convolutional Neural Network](https://arxiv.org/abs/1609.02132)
  1. (Background) [Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks](https://arxiv.org/abs/2206.08916)

  
Mar. 2, Thu
: Transformer and its variants
  : Dong Keun, Yuan, and David
: 1. [Swin Transformer](https://arxiv.org/abs/2103.14030)
  1. [Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)
  1. (Background) [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
  1. (Background) [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)
  1. (Background) [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)
  1. (Background) [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
  1. (Background) [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)  


Mar. 7, Tue
: Self-supervised and Multimodal Learning
  : Chen Sun
: 1. (Background)[Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/) by Lilian Weng
  1. (Background)[Contrastive Representation Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/) by Lilian Weng
  1. (Background)[Self-Supervised Learning](https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf) by Andrew Zisserman


Mar. 9, Tue
: Self-supervised Learning for NLP
  : Yang, Adrian, and Vignesh
: 1. [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)
  1. [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827)
  1. (Background) [Self Supervision Does Not Help Natural Language Supervision at Scale](https://arxiv.org/abs/2301.07836)
  1. (Background) [How does in-context learning work?](https://ai.stanford.edu/blog/understanding-incontext/)
  1. (Background) [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529)
  1. (Background) [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
  1. (Background) [Human Language Understanding & Reasoning](https://www.amacad.org/publication/human-language-understanding-reasoning)
  1. (Background) [Do Large Language Models Understand Us?](https://www.amacad.org/publication/do-large-language-models-understand-us)


Mar. 10, Fri
: **Due**{: .label .label-purple} Final project signup


Mar. 14, Tue
: **Invited**{: .label .label-purple} Guest Lecture
  : [Sara Beery](https://beerys.github.io/)


Mar. 16, Thu
: Self-supervised Learning for Images and Videos
  : Arthur, Robert, and Siyang
: 1. [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)
  1. [Time-Contrastive Networks: Self-Supervised Learning from Video](https://arxiv.org/abs/1704.06888)
  1. (Background) [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)
  1. (Background) [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)
  1. (Background) [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
  1. (Background) [Deep Clustering for Unsupervised Learning of Visual Features](https://arxiv.org/abs/1807.05520)
  1. (Background) [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)
  1. (Background) [Learning image representations tied to ego-motion](https://arxiv.org/abs/1505.02206)


Mar. 21, Tue
: **Invited**{: .label .label-purple} Gues Lecture
  : [Ce Liu](http://people.csail.mit.edu/celiu/)


Mar. 23, Thu
: Project proposal

Mar. 28, Tue
: _Spring break_

Mar. 30, Thu
: _Spring break_

Apr. 4, Tue
: Reinforcement Learning
  : Chen Sun

Apr. 6, Thu
: World Models
  : Ray, Alexander Halpin, and Zhiyuan
: 1. [World Models](https://arxiv.org/abs/1803.10122)
  1. [Learning Latent Dynamics for Planning from Pixels](https://arxiv.org/abs/1811.04551)
  1. (Background) [Mastering Diverse Domains through World Models](https://arxiv.org/abs/2301.04104v1)
  1. (Background) [Control-Aware Representations for Model-based Reinforcement Learning](https://arxiv.org/abs/2006.13408)
  1. (Background) [Shaping Belief States with Generative Environment Models for RL](https://arxiv.org/abs/1906.09237)


Apr. 7, Fri
: **Due**{: .label .label-purple} Project proposal

Apr. 11, Tue
: Learning from Offline Demonstration
  : Anirudha, Zilai, and Akash
: 1. [Learning to Act by Watching Unlabeled Online Videos](https://cdn.openai.com/vpt/Paper.pdf)
  1. [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)
  1. (Background) [Building Open-Ended Embodied Agents with Internet-Scale Knowledge](https://minedojo.org/)
  1. (Background) [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
  1. (Background) [Understanding the World Through Action](https://arxiv.org/abs/2110.12543)
  1. (Background) [Learning Latent Plans from Play](https://arxiv.org/abs/1903.01973)



Apr. 13, Thu
: RL from Human Feedback
  : Ziyi, Qi, and Christopher
: 1. [Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)
  1. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
  1. [Why does ChatGPT constantly lie?](https://noahpinion.substack.com/p/4e262415-6b0e-41b7-ba2d-8f620790bf63) by Noah Smith
  1. [ChatGPT Is a Blurry JPEG of the Web](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) by Ted Chiang


Apr. 18, Tue
: Model Interpretability
  : Michael Lepori, Haoyu, and Qinan
: 1. [Do Vision Transformers See Like Convolutional Neural Networks?](https://arxiv.org/abs/2108.08810)
  1. [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/abs/2111.09259)
  1. (Background) [BERT rediscovers the classical NLP pipeline](https://arxiv.org/abs/1905.05950)
  1. (Background) [Concept Bottleneck Models](https://arxiv.org/abs/2007.04612)
  1. (Background) [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062)
  1. (Background) [Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/abs/2211.12312)
  1. (Background) [Neural Networks and the Chomsky Hierarchy](https://arxiv.org/abs/2207.02098)


Apr. 20, Thu
: Generative Modeling
  : Chen Sun


Apr. 25, Tue
: 3D Generation
  : Nitya, Linghai, and Yuan
: 1. [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988)
  1. [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751)
  1. (Background) [Text-To-4D Dynamic Scene Generation](https://arxiv.org/abs/2301.11280)


Apr. 27, Thu
: Compositionality
  : Lingze, Suraj, and Apoorv
: 1. [Learning to Compose Neural Networks for Question Answering](https://arxiv.org/abs/1601.01705)
  1. [Compositional Visual Generation with Composable Diffusion Models](https://arxiv.org/abs/2206.01714)
  1. (Background) [CREPE: Can Vision-Language Foundation Models Reason Compositionally?](https://arxiv.org/abs/2212.07796)
  1. (Background) [COGS: A Compositional Generalization Challenge Based on Semantic Interpretation](https://aclanthology.org/2020.emnlp-main.731/)
  1. (Background) [Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems](https://arxiv.org/abs/2205.01128)


Apr. 28, Fri
: **Due**{: .label .label-purple} Presentation slot signup

May 2, Tue
: _Final project office hours_

May 4, Thu
: _Final project office hours_

May 12, Fri
: Final project presentations

May 12, Fri
: **Due**{: .label .label-purple} Project submission
